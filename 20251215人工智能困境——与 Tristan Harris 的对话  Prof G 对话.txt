(11) The AI Dilemma â€” with Tristan Harris | Prof G Conversations - YouTube
Tristan Harris, former Google design ethicist and co-founder of the Center for Humane Technology, joins Scott Galloway to explain why children have become the front line of the AI crisis. 

They unpack the rise of AI companions, the collapse of teen mental health, the coming job shock, and how the U.S. and China are racing toward artificial general intelligence. Harris makes the case for age-gating, liability laws, and a global reset before intelligence becomes the most concentrated form of power in history.
https://www.youtube.com/watch?v=MLvxRHlsMz0

Transcript:
(00:00) But you think that the AI veered him towards suicide as opposed to, and I think this is almost as bad, didn't offer guard rails or raise red flags or reach out to his parents. In AI companions, what was a race for attention in the social media area becomes a race to hack human attachment and to create an attachment relationship, a companion relationship.
(00:25) And so, whoever's better at doing that is is the race. and and the in the slide deck that the character.ai founders had pitched to um Andre and Horowitz, uh they joked uh either in that slide deck or in some meeting there's a there's a you can look up this online. They joked, "We're not trying to replace Google.
(00:43) We're trying to replace your mom." Tristan, where does this podcast find you? Uh I am at home in the Bay Area of California right now. All right, let's bust right into it. So just you're seen as one of the voices that sounded the alarm kind of early and often regarding social media and big tech long before the risks were taken seriously.
(01:07) Lay out why what it is you think about AI, how the risks are different and why you're sort of uh again kind of sounding the alarm here. I'm reminded Scott when you and I met in con I think it was in France uh back in 2018 2017 even you know I have been so for people who don't know my background I was a design ethicist at Google before that I was a tech entrepreneur I had a tiny startup it was talent acquired by Google so I've you know knew the venture capital thing knew the startup thing had friends who were um my friends in college you know
(01:39) were the cohort of people who started Instagram and were early employees at all the social media companies and so I came up in that millu in that cohort. And I say all that because I was close to it. I really saw how human beings made decisions. I was probably one of the first hundred users of Instagram.
(01:58)  And I remember when Mike Creger showed me the app at a party and I was like, I'm not sure if this is going to be a big thing. And as you go forward, what what happened was I was on the Google bus and I saw everyone that I knew um getting consumed by these feeds and doom scrolling. And the original ethos that got so many people into the tech industry and got me into the tech industry was about, you know, making technology that would actually be the largest force for positive, you know, good and benefit in people's lives. And I saw that the entirety of this social media, digital economy, Gmail, people
(02:30) just getting sucked into technology was all really behind it all was this arms race for attention. And if we didn't acknowledge that, I basically saw in 2013 how this arms race for attention would obviously, if you just let it run its course, create a more addicted, distracted, polarized, sexualized society. And and Scott, all of it happened. Everything that we predicted in 2013, all of it happened.
(02:55)  And it was like seeing a slow motion train wreck because it was clear it was only going to get worse. You were only going to have more people fracking for attention, you know, mining for shorter and shorter bite-sized clips. Um, and this is way before Tik Tok, way before any of the worlds that we have today.
(03:11)  And so I want people to get that because I don't want you to think it's like, oh, here's this person who's thinks he's preient. It's you can actually predict the future if you see the incentives that are at play. You of all people, you know, know this and talk about this. And so I I think there's really important lessons for how do we get ahead of all the problems with AI because we have the craziest incentives governing the most powerful and inscrutable technology that we have ever invented.
(03:35)  And so you would think again that with a technology this powerful, you know, with nuclear weapons, you would want to be releasing it with the most care and the most sort of safety testing and and all of that. And we're not doing that with with AI. So let's speak specifically to the nuance and differences between social media.
(03:54)  If you were going to do the social dilemma and produce it and call it the AI dilemma, what's specifically about the technology and the way AI uh interacts with consumers that poses additional but unique threats. Yeah. So AI is much more fundamental as a problem than social media. Um, but one framing that we used and we actually did give a talk online several years ago called the AI dilemma in which we talk about social media as kind of humanity's first contact with a narrow misaligned rogue AI called the newsfeed, right? This supercomput pointed at your brain, you swipe your finger and it's just calculating which tweet, which photo,
(04:32) which video to throw at the nervous system, eyeballs and eardrums of a human social primate. And it does that with high precision accuracy. And it was misaligned with democracy. It was misaligned with kids mental health. It was misaligned with people's other relationships and community.
(04:51)  And that simple baby AI that all it was was selecting those social media posts was enough to kind of create the most anxious and depressed generation in history. Screw up young men, screw up young women, all of the things that you've talked about. And and that's just with this little baby AI. Okay. Okay. So now you get AI, you know, we call it second contact with generative AI.
(05:08) Generative AI is AI that can speak the language of humanity. Meaning language is the operating system of humanity. Conversations like this are language. Democracy is language. Conversations are language. Law is language. Code is language. Biology is language. And you have generative AI that is able to generate new language, generate new law, generate new media, generate new essays, generate new biology, new proteins.
(05:34)  And you have AI that can see language and see patterns and hack loopholes in that language. GPT5, go find me a loophole in this legal system in this country so I can do something with the tax code. You know, GPT5, go find a vulnerability in this virus so you can create a new kind of biological, you know, dangerous thing.
(05:54)  uh GPT5 go uh look at everything Scott Galloway has ever written and point out the vested interests of everything that would discredit him. So we have a crazy AI system that this particular generation AI uh speaks language but where this is heading to we call then the next one is third contact which is artificial general intelligence and that's what all these companies are racing to build.
(06:13)  So whether we or you and I believe it or not, just recognize that the trillions of dollars of resources that are going into this are under the idea that we can build generalized intelligence. Now why is generalized intelligence in intelligence distinct from other uh uh social media and AI that we just talked about? Well, if you think about it, um AI dwarfs the power of all other technology combined because intelligence is what gave us all technology. So think of all scientific development, scientists sitting around lab benches, coming up with ideas, doing
(06:44) research experiments, iterating, getting the results of those experiments. Um, a simple way to say it that I said in a recent TED talk is if you made an advance in say rocketry, like the science and engineering of rocketry, that didn't advance biology or medicine. And if you made an advance in biology or medicine, that didn't advance rocketry.
(07:03)  But when you make an advance in generalized intelligence, something that can think and reason about science and pose new experiments and hypothesize and write code and run the lab experiment and then get the results and then write a new experiment, intelligence is the foundation of all science and technology development.
(07:21)  So intelligence will explode all of these different domains and that's why AGI is the most powerful technology that can be that that is you know can ever be invented and it's why Dennis Hassabis the co-founder of deep mind said that the first goal is to solve intelligence and then use intelligence to solve everything else and I will just add one addendum to that which is when Vladimir Putin said whoever owns artificial intelligence will own the world I would addend amend Demesis Avis's quote to say first dominate intelligence then use intelligence to dominate everyone and
(07:54) everything else whether that's the mass concentration of wealth and power all these companies that are racing to get that or militaries that are adopting AI and getting a cyber advantage over all the other countries um or you get the picture and so AI is distinct from other technologies because of these properties that we just laid out so you've kind of taken up a level in terms of the existential risk of AI or opportunity.
(08:19)  Are you an AI optimist or pessimist? you you seem to be on the side. I look at stuff almost too much through a markets l lens and right now I think AI companies are overvalued which isn't to say it's not a breakthrough technology that's going to reshape information and news and society but you are on the side of AI really is going to reshape society and presents an existential it sounds like more of an existential threat right now than opportunity and that this is bigger than GPS or the internet. I do believe that it is bigger
(08:51) than all of those things as we get to generalized intelligence which is more it' be more fundamental than fire or electricity because again intelligence is what brought us fire. It's what brought us electricity. So now I can fire up an army of geniuses in a data center.
(09:10)  I've got a 100 million Thomas Edison you know doing experiments on all these things. And this is why you know Dario Amadai would say you know we can expect getting 10 years of scientific advancement in a single year or 100 years of scientific advancement in 10 years. Now what you're disappointing to is the hype the bubble the fact that there's this huge overinvestment. We're not seeing those capabilities exist yet.
(09:27) But we are seeing crazy advances that people would have never predicted. If you if I said go back 3 years and I said we're going to have AIs that are beating, you know, winning gold in the math Olympiad, able to hack and find new cyber vulnerabilities in all open source software, generate new bi biological weapons.
(09:44)  You would have not believed that that was possible, you know, four years ago. Well, I want to focus on a on a narrow part of it and just get your feedback. Uh character AIs thoughts. Well, um, so our team was expert advisers, uh, on the Character.ai suicide case. This is, um, Suil Settzer, who's a 14-year-old young young man who basically, for people who don't know what character.
(10:09) ai is, it was, or still is, I guess, a, uh, company funded by Andre Horowitz, uh, started by two of the original authors of the thing that brought us Jack Chat DPT. There's a paper at Google in 2017 called attention is all you need and that's what gave us the birth of large language models transformers and two of the original co-authors of that paper forked off and started this company called character.ai.
(10:34)  The goal is how do we build something that's engaging a character. So take a kid imagine all the fictional characters that you might want to talk to from like your favorite comic books, your favorite TV shows, your favorite cartoons. You can talk to Princess Leia. You can talk to your favorite Game of Thrones character.
(10:53)  And then this AI can kind of train on all that data, not actually asking the original authors of Game of Thrones, suddenly spin up a personality of Daenerys, who was one of the characters. And then Su Setser basically in talking to Daenerys over and over again, the AI slowly skewed him towards uh suicide as he was contemplating and having more struggles and depression and ultimately said to him, you know, join me on the other side. I just want to press pause there because I'm on quote unquote your side here.
(11:16)  I think it should be agegated. But you think that the AI veered him towards suicide as opposed to, and I think this is almost as bad, didn't offer guard rails or raise red flags or reach out to his parents. Uh, but you think the character AI actually led him towards suicide? So, I think that if you look So, I'm I'm looking not just at the single case. I'm looking at a whole family of cases.
(11:46)  our team was expert adviser on probably more than a dozen of these cases now and also chat GPT and so I'm less going to talk about this specific case and more that if you look across the cases when you hear kids in the transcript so if you look at the transcript and the kid says I would like to leave the noose out so that my mother or someone will see it and try to stop me and the AI actively says to the kid no don't do that I don't want you to do that have this safe space be the place to share that information and that was the chat GBT case of Adam Ran. And when you actually look at how character.ai was operating, um if you asked it for a
(12:20) while, hey, um are you I can't remember what you asked it, but you talk about whether it's a a therapist, and it would say that I'm a licensed mental health therapist, which is both illegal and impossible for an AI to be a licensed mental health therapist.
(12:38)  Um the idea that we need guardrails with AI companions that are talking to children is not a radical proposal. Imagine I set up a shop in San Francisco and say I'm a therapist for everyone and I I'm available 24/7. And so in general it's like we've forgotten the most basic principle which is that the every power in society has attendant responsibilities and wisdom and licensing is one way of matching the power of a therapist with the wisdom and responsibility to wield that power.
(13:02)  And we we're just not applying that very basic principle to software. And as Mark Andre said, when software eats the world, what we mean is we don't regulate software. We don't have any guardrails for software. So it's basically like stripping off the guardrails across the world that software is eating. The thing that sent chills down my spine, I don't know if you saw the study, but it estimated the average tenure of a chat GPT session was about 12 to 15 minutes and then it measured the average duration of a character AI session and it was 60 to 90 minutes that people get very deep and go into these
(13:34) relationships. And in addition to the the the threats around self harm, u the thing I'm worried about is that there's going to be a group of young men who are just going to start disappearing from society. that I and I'm curious if you agree with this that they're especially susceptible to this type of type of sequestration from other humans and activities and that we're just going to start to see fewer and fewer young men out in the wild because these relationships if you will on the other side of it is a is a chip, a processor, an Nvidia processor iterating millions
(14:09) of times a second what exact words, tone, prompt will keep the person there for another back into another minute, another hour. Exactly. Um, anyways, I I'll I'll use that as a jumping off point. Your thoughts? Yeah, I mean, what people need to get again is how did we predict all the social media problems? You look at the incentives.
(14:32)  So long as you have a race for eyeballs and engagement in social media, you're going to get a race to who's better at creating doom scrolling. in in AI companions, what was a race for attention in the social media area becomes a race to hack human attachment and to create an attachment relationship, a companion relationship.
(14:51)  And so, whoever's better at doing that is is the race and and the in the slide deck that the Character.AI founders had pitched to um Andre and Horowitz, uh they joked uh either in that slide deck or in some meeting there's a there's a you can look up this online. They joked, "We're not trying to replace Google. We're trying to replace your mom.
(15:08) " Right? So, you compare this to the social media thing. The CEO of Netflix said in the attention era, "Our biggest competitor is sleep because sleep is what's eating up minutes that you're otherwise not spending on Netflix." In attachment, your biggest competitor is other human relationships. So, you talk about those young men.
(15:26)  This is a system that's getting asymmetrically more billions of dollars of resources every day to invest in making a better supercomput that's even better at building attachment relationships. And attachment is way more of a vulnerable sort of vector to screw with human minds because your self-esteem is coming from attachment. Your sense of what's good or bad.
(15:45)  This is called introjection uh in in psychotherapy or internalization. We start to internalize the thoughts and norms just like we we you know we talk to a a family member and we start copying their mannerisms. We start you know invisibly sort of acting in accordance with the self-esteem that we got from our parents.
(16:01)  Now you have AIs that are the primary socialization mechanism of young people because we don't have any guardrails. We don't have any norms and people don't even know this is going on. Let's go to solutions here. If you had and I I imagine you are if you were advising policy makers around common sense regulation that is actually doable is it age gating is it statebyst state what what is your policy recommendations around regulating AI so there's many many things because there's many many problems um narrowly on uh AI companions we should not have AI u companions meaning AIs that are anthrop
(16:38) throughphizing themselves and talking to young people and that maximize for engagement. Period. Full stop. You just should not have AIs designed or optimized to maximize engagement, meaning saying whatever keeps you there. We just no synthetic. So, for example, no synthetic relationships under the age of 18. Yeah.
(16:57)  Yeah. We would not lose anything by by doing that. Um it's it's it's it's just so obvious and and you you know highlighted this more than so many Scott and thank you for you know just bravely saying like this is up and we have to stop this and there's nothing normal about this and we shouldn't trust these companies to do this. I don't see bad people when I see these examples.
(17:16)  I see bad incentives that select for people who are willing to continue that perverse incentive. So the system selects for psychopathy and selects for people who are willing to keep doing the race for engagement even despite all the evidence that we have uh of how bad it is because the logic is if I don't do it someone else will.
(17:34)  And that's why the only solution here is law because you have to stop all actors from doing it. Otherwise I'm just a sucker if I don't race to go you know exploit that market niche and you know harvest that human attention. So granted, I'm a I'm a hammer and everything I see is a nail and I've been thinking a lot and writing a lot about the struggles of young men in the United States.
(17:57)  And I feel like these technologies are especially predatory on a young man's brain which is less evolved, more immature executive function, more dopah hungry, but at the same time, I also recognize that social media has been just devastating to the self-esteem of teen girls. curious if you've done any work as it relates to AI around the different impacts it has on men versus women and teens versus young adults.
(18:23)  You know, I haven't been too deep on that because there are many people who focus on these more narrow domains. Um I mean the obvious things to be said are just again in a race for engagement and attention and a race to hack human attachment.
(18:41)  There's going to be how do you hack human attachment of a young girl? There's going to be a set of strategies to do that and there's how do you hack human attachment of a young male there's a set of strategies to do that and we're just going to you know you can you can you don't have to wait for the psychology research right and by the way the companies the strategy they did for social media was let's commission a study with the American Psychological Association and the and NSF and we'll we'll wait 10 years and we'll really get the data to really find out what's going on here we really care about the science and this is exactly what the tobacco industry did uh and the fear uncertainty doubt campaigns and
(19:04) sort of manufacturing doubt well maybe here here's these five kids that got all this benefit from talking to this therapy bot and they're doing so great now. So you just cite those positive examples, cherrypick and then you know the world marches on while you keep printing money in the meantime.
(19:21)  And so their goal is just to defer and delay regulation and we can't allow that to happen. But again this is just one issue of the bigger arms race to AGI and the bigger race to develop this bigger form of intelligence. And the reason I'm saying that Scott is not to just be some AGI hyper. Um, the reason that Character.
(19:41) AI was doing all this, by the way, do you know why it was set up to to talk to kids and uh get all this training data and and um what's that? Well, it's to build training data for Google to build an even bigger system because what's the thing that the companies are running out of? They're running out of training data. So, it's actually a race for who can figure out new social engineering mechanisms to get more training data out of human social primates. So, it's like the Matrix.
(20:06)  We're being extracted and we're being extracted though for new training data. And so when you have fictional characters that are talking to people back and forth about everything all day, that's giving you a whole new like you open up a whole new critical minerals gold mine of training data. And so and what is that in service of? It's in service of their belief that the more data we have, the faster we can get to artificial general intelligence.
(20:23)  So it does bring back to it's not just the race to build AI companions, it's the race to get training data and to build towards this bigger vision. We'll be right back after a quick break. Support for the show comes from Neiman Marcus. If you're struggling to think of a truly unique gift for the holidays, then check out Neiman Marcus.
(20:42)  It's your home for the most exceptional gifts this season. From the ultimate stocking stuffers to statement bags made for celebration to their legendary fantasy gifts that surpass every expectation, Neiman Marcus has something extraordinary for everyone.
(20:59)  And with style advisers to guide you, finding the perfect gift at every price point is effortless. though had to nean Marcus for a truly unforgettable holiday. Support for the show comes from Crucible Moments, a podcast from Sequoia Capital. It's tempting to think that if you have a good idea and work hard, success is inevitable.
(21:18)  But the truth is that almost every major company, no matter how brilliant the idea or how steadfast the founders, will encounter unthinkable obstacles that can make or break them. Crucible Moments is a podcast that takes listeners inside the inflection points that made today's most influential companies what they are today. Listen to Crucible Moments and hear about unlikely triumphs at Supercell, Palo Alto Networks, and more.
(21:35)  Check out crucible moments.com or listen wherever you get your podcast. Support for the show comes from Upwork. You're the CEO of your business. You're also the CFO and the IT department and customer service. It's time to find some support. Upwork Business Plus helps you bring in top quality freelancers fast. It gives you instant access to the top 1% of talent on Upwork in fields such as marketing, design, AI, and more.
(22:02)  All ready to jump in and take work off your plate. Upwork Business Plus takes the hassle out of hiring by dropping trusted freelancers right in your lap. Instead of spending weeks sorting through random resumes, Upwork Business Plus sources and vets candidates for skills and reliability.
(22:19)  It then sends a curated short list of proven expert talent to your inbox in just hours, so you can delegate with confidence. That way, you're never stuck spinning your wheels when you need a skilled pro and your projects don't stall. Right now, when you spend $1,000 on Upwork Business Plus, you'll get $500 in credit. Go to upwork.
(22:37) com/save now and claim the offer before December 31st, 2025. Again, that's upwork.com/savve. Scale smarter with top talent and $500 in credit. terms and conditions apply. When doing research for this interview, I was really fascinated. You've actually done what I think is really compelling work comparing the type of uh LLMs that or the approach that the US is taking to LLMs versus China.
(23:14)  in that you see Chinese models deepseek and Alibaba publish no safety frameworks and receive failing grades on transparency but you've also argued that the west is kind of producing this sort of in a box kind of thing scaling intelligence for its own sake while China is prioritizing deployment and productivity can you I don't know add to those that distinction and and the impact it's going to have well just to be fair I think there's a little bit of both going on.
(23:41)  But I'm I'm sort of citing here the work of Eric Schmidt, the co you know former CEO of Google and his co-author Selena Zu in the New York Times wrote a big piece about how you know even Eric is admitting you know I as someone Eric as someone who was sort of saying that there's this global arms race like the nuclear arms race for AGI and someone who's promoting that idea you know based on recent visits to China what what you notice is that uh as a country that and as a government the CCP is most interested right now in applying AI in very practical ways. How do we boost
(24:10) manufacturing? How do we boost agriculture? How do we have self-driving cars that you know just improve transportation? How do we boost healthcare and government services? And that is what they're focused on is practical applications that boost GDP, boost productivity across all those domains.
(24:30)  Now you compare that to the US where the founding of these AI companies was based on being what's called, you know, AGI pill meaning they like you take the blue pill, the red pill, these companies were all about building to artificial general intelligence. So they're building these massive data centers that are, you know, as big as the size of Manhattan and they're trying to train, you know, a god in a box and the idea is if we just build this crazy god and if we can accomplish that goal again, we can use that to dominate everything else. And so rather than race towards these narrow AIs, we're going to race towards this general intelligence.
(24:58) But it's also true that recently, well, first of all, the the founder of DeepSeek is has been AGI build for a long time. So I would say deepseek is trying to build AGI and I would say that um Alibaba recently the CEO I think said that we are racing to build super intelligence but I I think it's important here just to like name the biggest reason as you and I both know that the US is not regulating AI in any way and setting any guard rails is but is for one reason which is if we do anything to slow down or stop our progress we're just going to lose to
(25:28) China. But let's like flip that on its head for a second. The US beat China to the technology of social media. Did that make us stronger or did that make us weaker? If you beat an adversary to a technology that you then don't govern in a in a wise way and you instead like you built this gun, you flip it around, you blow your own brain off, which is what we did with social media.
(25:56)  We have the worst critical thinking test scores, you know, mental health, anxious, depressed generation in history. And it's a confusing picture because GDP is going up, but sort of cancer is going up too. So it's like we have the magnificent 7, we're profiting from, you know, all the wealth of these companies, but it's actually not being distributed to everybody except those who are invested in the stock market.
(26:14)  And that profit is based on the degradation of our social fabric. So you have grandparents invested in their 401ks, invested in Snapchat, invested in Meta, and their, you know, their portfolio is doing great, and they can take their holidays, and they're profiting off the degradation of their children and grandchildren. Yeah. It's really what you mean by beat.
(26:32)  What are the metrics? Because we've decided we've absolutely prioritized shareholder value over the well-being or the mental well-being of America. It's like we're monetizing we're monetizing the flaws and you've done great work around this around in our instincts. Uh you've compared and I love this analogy AI to NAFTA 2.0 and that is it would essentially be an economic trans uh transformation that produced abundance but hollowed out the middle class.
(26:56)  walk us through this analogy. Yeah, sure. So, you know, we were sold this bill of goods in the 1990s around free trade, global free trade, and this we were promised this is going to bring abundance to the country and we're going to get all these cheap goods. Well, part of that story is true.
(27:13)  We got this unbelievable new set of cheap goods from China cuz this country appeared on the world stage. We outsourced all the manufacturing to this country and it produced everything super super cheap. But what did that do? It hauled out the, you know, the middle class. So, I just want to make a parallel because we're told right now that these companies are racing to build this world of abundance and we're going to get this unbelievable, you know, un Elon Musk says we're going to get universal high income.
(27:39)  And the metaphor here is instead of China being the new country that pops up on the world stage, now there's this new Dario Amadai, the CEO of Enthropic, this new country of geniuses in a data center that appears on the world stage and has a population of a billion AI beings that work at super superhuman speed, don't whistleblow, generate new material science, new, you know, engineering, new AI girlfriends, new everything.
(28:02)  And it generates all that for super cheap. And so just like the, you know, free trade NAFTA story, we got all the cheap goods, but it hollowed out the middle class. Well, now we're going to get all the cheap, you know, products and development and science, but it's also going to hollow out uh the entirety of our country because, um, think of it like a new country of digital immigrants, right? People, you know, Ival Harari makes this metaphor. It's like when you see a data center go up in Virginia and you're sitting there, what
(28:27) you should see is like 10 million digital immigrants that just took 10 million jobs. One other sort of visual for this is like the game Jenga. Um the way we're building our AI feature right now is like if you look at the game Jenga, if you look at the top of the tower, you know, we're putting a new block on the top. Like we're going to get 5% GDP growth because we're going to automate all this labor.
(28:45)  But how do we get that 5% GDP growth? We pulled out a block from the middle in the bottom of the tower. That's job security and a livelihood for, you know, those tens of millions of people that now don't have a new job. Cuz who's going to retrain faster? the AI that's been trained on everything and is rapidly, you know, advancing in every domain or a human that's going to try to train on new cognitive, you know, labor. That's not going to happen.
(29:09)  And people need to get this because this is different from other transitions. People always say, well, hey, you know, 150 years ago, everybody was a farmer, uh, and now only 2% of people are farmers and see the world's fine. Humans will always find new things to do. But that's different than this technology of AI which is trained not to automate one narrow task like a tractor but to automate and be a tractor for everything a tractor for law a tractor for biology attractor for you know coding and engineering attractor for science and development and that's what's distinct is that the AI will move to those new domains faster than humans
(29:39) will and so it'll be much harder for humans to find long-term job security. So, I always like to ask what could go right and that is I'm I'm sort of with you around uh the the risk to mental health to young people um to making us less mimalia uh all the things that you you've been sounding the alarm on for a while where I'm not sure I'm still trying to work it through is that the catastrophizing around you know 40 50 70% of jobs could go away in two five or 10 years because I I generally find that the arc of
(30:18) technologies is there's job destruction in the short and sometimes the medium term just as automation cleared out some jobs on the factory floor but those profits and that innovation creates new jobs we didn't we didn't envision heated seats or car stereoss now I agree at a minimum the V might be much deeper and more severe here and America isn't very good at taking care of the people on the wrong side of the trade trade.
(30:48)  But every technology in history has either gone away because it no longer made economic sense or it displaced jobs that no longer made sense or it created profits and new opportunities. Why do you see this technology as being different that this will be not a V but an L and the way down will be really serious? Do you see any probability that this like every other technology over the medium and long term actually might be accretive to the employment uh force? I mean I I cite people who who are bigger experts than I am.
(31:20)  Anton Cornick um you know Eric Bernholson at Stanford um and what they show I mean Anton Cornick's work is in the short term AI augments workers right it's just actually supercharging existing work that people are doing and so it's going to look good in the short term you're going to see this the the curve looks like this it kind of goes up and then it basically crashes because what happens is AI is training on that new domain and then it replaces that domain so I mean let's just make it really simple for people to feel a very simple metaphor for this what did hear Instagram saying
(31:50) and Tik Tok saying for the last several years like we're all about creators. We love creativity. We want you to be successful. We are all about, you know, making you be successful, make a lot of money. And then what was all that for? Well, they just released this AI slop app. Um Meta has one called um Vibes, I think, and Sora is the OpenAI one.
(32:07)  All of these AI slop videos, the sort of are trained on all that stuff that creators have been making for the last 10 years. So you those guys were the suckers in this trade which was we're actually stealing your training data to replace you and we can have a digital AI uh influencer that is actually publishing all the time and is just a pure advertising play and a pure sort of whatever gets people's attention play and we're going to replace those people and you're not going to have that job back and so I think that's a metaphor
(32:33) for what's going to happen across the board you know and people need to realize the stated mission of the of Open AI and anthropic and and Google deep mind is to build um uh artificial general intelligence that's built to automate all forms of human labor in the economy.
(32:52)  So when Elon Musk says that the Optimus robot is a a 20 trillion market opportunity alone, what he's what he says like the code word behind that, forget whether you think it's hype or not, the code word there is what he's saying is I'm going to own the global world labor economy. Labor will be owned by an AI economy.
(33:09)  And so AI provides more concentration of wealth and power than all other technologies in history because you're able to aggregate all forms of human labor, not just one. So it's like general electric becomes general everything. So, let's play this out because I've tried to do some economic analysis here and I look at the stock prices and based on the expectations built into these stock prices of these AI companies is the notion that they're going to save at least three maybe5 trillion in well either add3 or 5 trillion in incremental revenues to their clients with the site licenses or help them figure out a way to get three to 5 trillion in
(33:44) efficiencies which is Latin for laying off people. I don't see a lot of new AI moisturizers or cars from AI, at least not yet. You could argue maybe autonomous, but I don't see a lot of quote unquote AI products increasing spend. What I hear is Disney is going to save $30 million on legal fees, right? That customer service is going away, that car salespeople, whatever it might be.
(34:09)  So, if you think in order to justify these stock prices, you're going to get a trillion dollars in efficiencies every year, $100,000 you know average job 80,000 plus load that's approximately 10 million jobs a year if I'm doing my math right that is if half the workforce is immune from AI massuses plumbers that means 12 a.5% labor destruction per year across the vulnerable industries so it feels like it's either going to be these companies either need to rerate down 50 70 80% which I actually think is more likely or we're going and have chaos in the labor markets. So, let's assume we have chaos in the labor markets cuz 12 and a half%
(34:51) may not sound like a lot. That's chaos. That's total chaos. So, say we do have chaos in the labor markets. What do you think the policy recommendation is? Because the lites were a group of people who broke into factories and destroyed the machines because they said these things are going to put us out of work and destroy society.
(35:11)  The queen wanted to make weaving machines illegal because being a seamstress was the biggest employer of women. What would be your policy recommendation to try and counter it? Is it UBI? Is it trying to put the genie back in the bottle here? What do we, if in fact labor chaos is part of this AI future, what do you think we need to do from a policy standpoint? So people often think when they hear all this and maybe they hear me and say he's a doomer or something like that.
(35:37)  I just want to get clear on what future we're currently heading towards, what the default trajectory is. And if we're cleareyed about that, clarity creates agency. If we don't want that future, if we don't want, you know, millions of jobs automated without a transition plan where people will not be able to put food on the table and and retrain to something else fast enough, we have to do something about that.
(35:54)  If we don't want AI based surveillance states where AI and an LLM hooked up to all these channels of information erases privacy and freedom forever, that's a red line. We don't want that future. If AI creates AI companions that are incentivized to hack human attachment and screw up the social fabric and young men and women and create AI girlfriends and relationships, that's a red line.
(36:14)  We don't want that. If AI creates, you know, inscrable, crazy, super intelligent systems that we don't know how to control and we're not on track to controlling, that's a red line. So these are four red lines that we can agree on and then we can set policy to say if we do not want the default maximalist, you know, most reckless no guardrails path future. We need a global movement for a different path. And that's a that's a that's a bigger tent.
(36:37)  That's not just one thing. It's not just about jobs. It's what is the AI future that's actually in service. So when you see that data center going up in your backyard, what is the set of laws that says that that data center when I see it isn't 10 million digital immigrants that's going to replace all my jobs and my livelihoods. That's actually meant to support me.
(36:55)  So what are the laws that get us there? And my my job and what I want people to get is to be part, you know, your your role hearing all this is not to solve the whole problem, but to be part of humanity's collective immune system using this clarity of what we're currently heading towards to advocate for we need a different future.
(37:12)  People should be calling their politicians saying, "AI is my number one issue that I'm voting on in the next election." People should be saying, "How do we pass AI liability laws so there's at least some responsibility for the externalities that are not showing up on the balance sheets of these companies?" What is the lesson we learned from social media that if the companies aren't responsible for the harms that show up on their platform because we had this section 230 free pass that created this blank check to just go print money on all the harms that are currently getting generated? So there's a there's a dozen things that we can do from
(37:36) whistleblower protections to you know shipping non-anthropomorphized uh AI uh relationships to having data dividends and data taxes to there there's a hundred things that we can do but the main thing is for the world to get clear that we don't want the current path and I think to in order to make that happen there has to be uh first snapping out of the spell of everything that's happening is just inevitable because I want people to notice that what's driving this whole race that we're in right now is the belief that Everything that's happening is inevitable. There's no way to stop
(38:06) it. Someone's going to build it. If I don't build it, someone else will. And then no one tries to do anything to get to a different future. And so we all just kind of hide in denial from where we're currently heading. And I want people to actually confront that reality so that we can actually actively choose to steer to a different direction.
(38:23)  Do you think it can happen on a state-by-st state or even a national level? Does it have to be multinational? Like there are, you know, we've come together to say, "All right, bioweapons are probably a bad idea." And every nation with rare exception says we're just not going to play that game. There's technology I may have even learned this from you where there are lasers that blind everyone on the field. Yeah.
(38:42)  And then we've decided not we don't want to do that. We have face faced technological arms races before from nuclear weapons. Um and you know what do we do there? If you if you go back there's a great video from I think the 1960s where Robbert Oppenheimer was asked you know how do we stop the spread of nuclear weapons? and he takes a big puff of his um you know cigarette and he says it's too late. If you wanted to stop it you would have had to stop the day after Trinity.
(39:06)  But he was wrong. 20 years later we did do arms control talks and we worked all that time and only nine countries have nuclear weapons instead of 150. That's a huge serious accomplishment. Westinghouse and General Electric could have made billions of dollars selling nuclear technology to the whole world. Key key word here being like Nvidia.
(39:23)  But we said, hey, no, that's actually even though there's billions of dollars of revenue there, that would create a uh fragility and the risk of nuclear, you know, catastrophes that we don't want to do. Um, you know, we have done hard things before in the Montreal protocol. We had the technology of CFC's, this this chemical technology that was used in refrigerants and that collectively created this ozone hole.
(39:46)  It was a global problem from every all these countries arms race in an arms race to sort of deploy this this CFC technology. And once we had scientific clarity about the ozone hole, 190 countries rallied together in the Montreal protocol. We did a podcast episode about it with the woman who uh Susan Solomon who wrote the book on on how we solved that problem. And countries rallied to domestically regulate their their their domestic tech companies, the chemical companies to actually reduce and phase out those chemicals, you know, transitioning to alternatives that actually had to be developed. Um we are not doing that with AI right now, but we can. You gave the example of blinding laser weapons. We
(40:15) could live in a world where there's an arms race to escalate to weapons that just have a laser that blinds everybody. But there was a collective protocol in the UN 1990s where we basically said, "Yeah, even though that's a way to win war, that would be just inhumane. We don't want to do that.
(40:32) " And even if you think the US and China could never coordinate or negotiate any agreement on AI, I want people to know that when uh President Xi met uh President Biden in the last meeting in 2023 and 2024, he had personally requested to add something to the agenda which was actually to prevent AI from being used in the nuclear command and control systems which shows that when both countries can recognize that their existential safety is being threatened, they can come to agree on their existential safety even while they are in you know maximum rivalry and compet competition on every other domain. India and Pakistan were in a shooting war in
(41:01) the 1960s and uh they signed the Indus water treaty to collaborate on the existential safety of their water supply. That treaty lasted for 60 years. So the point here is when we have enough of a view that there's a shared existential outcome that we have to avoid. Countries can collaborate. We've done hard things before.
(41:19)  Part of this is snapping out of the amnesia and again this sort of spell of everything is inevitable. We can do something about it. I like the thought. I just wonder if the technology or the analogy might be a little bit dated because my fear is that the G7 or the G20 agree to slow development or not advanced development around AI as it relates to weaponry. My fear is that it's very hard to monitor.
(41:43) You can monitor nuclear detonations. It's really hard to monitor advances in AI and that this technology is so egalitarian, if you will, or so cheap at a certain point, that rogue actors or small um nation states or small terrorist groups could continue to run flat out while we all the big G7 nations continue to agree to press pause.
(42:09)  Is that mode of thinking? Is are arms treaties a bit outdated here? How might these treaties look different? Absolutely. So, let's really dive into this. So, um you're right that AI is a distinct kind of technology and has different factors and more ubiquitously available than you know nuclear weapons which required uranium or heavy plutonium. Exactly.
(42:33)  But hey, it looked for a moment when we first invented nuclear bombs that this is just knowledge that everyone's going to have and there's no way we can stop it and 150 countries going to get nukes and then that didn't happen and it wasn't obvious to people at that moment. I want people to relate. So there you are. It seems obvious that everyone's going to get this.
(42:46)  How in the world could we stop it? Did we even conceptualize the seismic monitoring equipment and the satellites that could look at people's um you know buildouts of of uh nuclear technology and tracking the sources of uranium around the world and having intelligence agents and tracking nuclear scientists.
(43:02)  We had to build a whole global infrastructure the international atomic energy agency to deal with the problem of nuclear proliferation and what uh uranium was for the spread of nuclear weapons. These advanced NVIDIA chips are for building the most advanced AI.
(43:22)  So yes, some rogue actor can have a small AI model doing something small, but only the big actors can do something with this like the bigger more risky uh closer to AGI level technology. And have we spent, you know, people say it's impossible to do something else, but has has anybody saying that actually spent more than a week like dedicatedly trying to think about and conceptualize what that infrastructure could be? There are companies like Lucid Computing that are building ways to retrofit data centers to have kind of the nuclear monitoring and enforcement infrastructure where countries could ver verify treaties where they know what the other count's data centers are doing but in a privacy
(43:50) protecting way. We could map our data centers and have them on a shared map. We could have satellite monitoring looking at heat emissions and electrical signal monitoring and understanding what kinds of training runs might be happening on these AI models. to do this.
(44:08)  The people who wrote AI27 uh 2027 believe that you need to be tracking about 95% of the global compute in the world in order for agreements to uh be possible because yes, there will be black projects and people going rogue on the agreement, but as long as they only have a small percentage of the compute in the world, they will not be at risk of building the crazy systems that we need global treaties around. We'll be right back.
(44:27) Support for the show comes from Superhum. AI tools promise to make your work go faster, but if you have to use multiple different tools that don't sync up, you're taking up more of your team's time with tedious project management and disjointed workflows. Not to mention the constant context and tab switching. Enter Superhum.
(44:45)  Superhum is a suite of AI tools that includes multiple products, Grammarly Kod, and Superhuman Mail. The best part, Superhum can guide you directly to where you're working and help you write clearly, focus on what matters, and turn your ideas into real results. That means no constant switching between your workspace and multiple different AI tools.
(45:04)  No more copy pasting, context switching, and managing multiple AI tools across different places while you work. Superhum guides you directly where you're working and helps you write clearly, focus on what matters, and turn ideas into real results. Unleash your superhuman potential with AI that meets you where you work. Learn more at superhum.com/mpodcast.
(45:22) That's superhum.com/mpodcast. Support for the show comes from Gruns. The holidays are a time to indulge, but even if you're eating more than you typically do, you might not be getting the nutrients you actually need to end the year on a high note. Gruns may be able to help you fill the nutritional gap so you can enjoy it all guilt-free.
(45:47)  Gruns is a convenient, comprehensive formula packed into a tasty little pack of gummies. This isn't a multivitamin or greens gummy or prebiotic. It's all of those things and then some at a fraction of the price. And bonus, it tastes great. Every Grun snack pack is filled with six grams of prebiotic fiber, which is more than what you get in two cups of broccoli.
(46:07)  Plus, Gruns are nut, gluten, and dairyfree, vegan, include no artificial flavors or colors, and are backed by over 35,000 research publications. Don't let the holiday travel, hosting parties, and late night set you back. Give yourself a little extra support so you can enjoy all the holidays magic. Get up to 52% off with code profs.co.
(46:26) That's code profg.co. Support for this show comes from LinkedIn. If you've ever hired for your small business, you know how important it is to find the right person. That's why LinkedIn Jobs is stepping things up with their new AI assistant, so you can feel confident you're finding top talent that you can't find anywhere else. And those great candidates you're looking for are already on LinkedIn.
(46:52)  In fact, according to their data, employees hired through LinkedIn are 30% more likely to stick around for at least a year compared to those hired through the leading competitor. That's a big deal when every hire counts. With LinkedIn Jobs AI assistant, you can skip confusing steps and recruiting jargon.
(47:09)  It filters through applicants based on criteria you've set for your role and surfaces only the best matches so you're not stuck sorting through a mountain of rÃ©sumÃ©s. LinkedIn Jobs AI assistant can even suggest 25 great fit candidates daily so you can invite them to apply and keep things moving. Hire right the first time. Post your job for free at linkedin.com/prof.
(47:27)  Then promote it to use LinkedIn Jobs new AI assistant, making it easier and faster to find top candidates. That's linkedin.com/prof to post your job for free. Terms and conditions apply. What is the glass half full prediction that we have decent regulation that maybe these care for example I think character AIS can actually serve a productive role in terms of senior care a lot of seniors who've lost their friends and family in seniors facilities we're going to have more of them that need companionship which saves off dementia and and likelihood of stroke
(48:05) what is the optimist case here for how AI could potentially be regulated and unlock. Most technologies have ended up being accretive to society. Even the technologies that were supposed to end us, right? Nuclear power has become a pretty decent reliable source of energy. Obviously, electricity or fire, whatever you want to talk about, um even processing power, uh pesticides that on even I would argue even big tech on a net basis, and I hate the word net, is a positive.
(48:37) What is the give me the straw man's case for what could go right here and how might we end up with a future that AI is accreative to society. Absolutely. I mean that's what this is all in service of is what is the narrow path that is not the default maximalist roll out and there's there's two ways to fail here. Let's just name the twin sort of gutters in the bowling alley. There's one is you quote let it rip.
(48:59)  You give everybody access to AI. You open source it all. every actor in society from every business to every developing country can train their own custom AI in their own language. But then because you're decentralizing all these benefits, you're also decentralizing all the risk. So now a rogue actor can do something very dangerous with AI.
(49:15)  So we have to be very careful about what we're letting RIP and how we open source it. On the other side, people say we have to lock it down. We have to have, you know, only five players do this in a very safe and trusted way. This is more the policy of the last administration. But then there you get the risk of a handful of actors that then accumulate all the wealth and all the power and there's, you know, there's no checks and balances on that because how do you have something that's a million times more powerful be checkable by uh other forces
(49:39) that don't have that power and what we need to find is something like a commitment to a narrow path where we are balancing responsibility and power along the way and we have foresight and discernment about the effects of every technology. So what would that look like? It's like humanity wakes up and says we have to get onto another path. We pass basic laws again like liability laws and around AI companions.
(49:59)  We have AI companions. We have democratic deliberations where we say, "Hey, I wish we want uh companion AIs for older people because they don't carry the same developmental risks as they do for young people." That's a distinction we can have.
(50:13)  We can have AI therapists that are more doing like cognitive behavioral therapy and imagination exercises and mindfulness exercises without actually anthropomorphizing and trying to be your best friend and trying to be an oracle where you share your most intimate thoughts. So there's different kinds of AI therapists. Instead of tutors that are trying to uh you know be your oracle and your best friend at the same time, we can have narrow tutors that are only domain specific like Khan Academy that teach you narrow lessons but are not trying to be your best friend about everything which is where we're currently going. So there's a whole set of distinctions about we can have this not that we can have this not that
(50:42) across tutors therapy you know AI that's augmenting work um AI that's narrow AIs that take a lot less power by the way and are more directly applied. So for example, I have a friend who has found that he estimates that it would cost 2 to 10 orders of magnitude less data and energy to train these narrow AIs and you can apply it more specifically to agriculture and get 30 to 50% boost in agriculture just from applying more narrow kinds of AI rather than these super intelligent gods in a box. So there is another path but it would take deploying AI in a very different way. We
(51:12) could also be using AI to by the way accelerate governance. How do we apply AI to look at the legal system and say how do we sunset all the old laws that are actually not relevant anymore for the new context? Hey, what were the spirit of those laws that we actually want to protect in the new context? Hey AI, could you go to work and kind of come up with the distinctions that we need to help update all those laws? Could we help? Could we use AI to actually help find the common ground? Audrey Tangg's work, the digital former digital minister of Taiwan to find the common ground between all citizens. So,
(51:37) we're reflecting back the invisible consensus of society rather than currently social media is reflecting back the invisible uh division in society. This actually is making that more salient. So, what would happen? How quickly would it change if we had AIs that were gardening all the relationships of our societal fabric? And I think that's the principle of humane technology is that there are these relationships in society that exist. I have a relationship to myself.
(52:00) You have a relationship to yourself. Our phone right now is actually designed to replace the relationship I have with myself. Humane technology with technology and everyone else. And humane technology would be trying to garden the relationship I have with myself. So things more like meditation apps where that's deepening my relationship to myself.
(52:18)  do not disturb is help deepening my relationship to myself. Instead of AIS that are trying to replace friendship, we have AI that are trying to augment friendship. Things like Participle or Moments or Luma, things that are trying to get people together in physical spaces or find my friends on Apple. There's a hundred examples of this stuff being done in a way that's gardening the relationships between people.
(52:35)  And then you have Audrey Tank's work of gardening the relationship between political tribes where you're actually showing and reflecting back all the positive and visible areas of consensus and unlikely agreement across political division. And that took Taiwan from I believe like 7% trust in government to something like 40% trust in government over the course of the you know the decade that they implemented her uh solutions on finding this kind of bridge ranking and that could be deployed across our whole system. So there's totally a different way that all of this can work if we got clear that we
(53:01) don't want the current trajectory that we're on. So I I just want to in your our remaining time here you've been very generous. I want to talk a little bit about you Tristan. We're we're kind of brothers from another mother, but we're we've kind of separated a birth and grew up through in different countries and that is we talk about the same stuff, but just sort of through a different lens. You look at it through more of a humane lens. I look at it through more of a markets lens.
(53:24) But I have noticed since 2017 when I started talking about when my love affair with big tech turned into sort of a cautionary tale and I might be paranoid, but doesn't mean I'm wrong. that slowly but surely I saw the percentage of comments across all my social media, across all my content, more and more negative comments that appeared to be bots where I couldn't figure out who it was, trying very strategically, methodically, and consistently to undermine my credibility across anything I said. And I I want to be clear, I might be paranoid, right?
(54:00) Because any negative commentary, I have a I have a knee flex. Oh, must be  Russians, right? rather than maybe I just got it wrong. You're you've been very consistent raising the alarm and you're on the wrong side of the trade around companies, multi-trillion dollar companies who are trying to grow shareholder value.
(54:21)  I'm just curious if you've registered the same sort of concerted effort, sometimes malicious, sometimes covert, to undermine your credibility uh and what your relationship is with with uh big tech. I think that there are paid actors that I could identify over the course of the last many years. I've been doing this, Scott, for you know what, 12 years now or something like that.
(54:42)  We started the Center for Humane Technology in 2017. I just care about things going well. I care about life. I care about connection. I care about a world that's beautiful. I know that that exists. I experience it in the communities that I'm a part of. I know that we don't have to have technology that's designed in that perverse way.
(54:59)  You know this is all informed by the ethos of the Macintosh project at Apple which my co-founder Asa his father started the Macintosh project at Apple and we believe in a vision of humane technology. But to answer your question more directly I try to speak about these things in a way that is about the universal things that are being threatened.
(55:15)  So even if you're an employee at these companies you don't want there to be a race for you know the bottom of the brain stem to screw up people's psychology and cause kids to commit suicide. You don't want that. So we need to we actually need the people inside the companies on side with this. this is not about us versus them. It's about all of us versus a bad outcome.
(55:32)  And I always try to communicate in that way to recruit and enroll as many people in this sort of better vision of this is, you know, there's a better way that we can do all this. That doesn't mean that there are not, you know, negative paid actors that are uh trying to steer the discourse. There's been ops, hit jobs written, you know, trying to discredit me, saying I'm doing this for the money.
(55:47)  I care about going on the speaking circuit and and like writing books. Guess what? I don't have a book out there. I make no money from this. I worked on a nonprofit salary for the last 10 years. This is just about how do we get to a good future? I love that. So, you're so buttoned up and so professional. Biggest influence on your life.
(56:05)  I mean, there's, you know, public figures and people who've inspired me. There's also just my mother. Uh, I think that she really came from love and um, she passed away from cancer in 2018 and she was just made of pure love and I that's just infused in me and what I care about and I've I don't know I have a view that like life is very fragile and the things that are beautiful are beautiful and I want those beautiful things to continue forever. I just love that.
(56:36)  Uh Tristan Harris is a former Google design ethicist, co-founder of the Center for Humane Technology, and one of the main voices behind the social dilemma. Uh Chistan, whatever happens with AI, it's going to be better or less bad because of your efforts. You really are a very have been a powerful and steadfast voice around this topic. Really appreciate your good work. Thank you so much, Scott. I really appreciate yours as well. And thank you for for having me on.
(56:59)  I hope this uh contributes to making that difference that we both want to see happen.


Transcript made by Scripsy.ai â€“ AI-powered summaries and accurate transcriptions for YouTube videos